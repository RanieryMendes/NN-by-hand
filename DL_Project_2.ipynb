{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project 2 \n",
    "\n",
    "* Due latest by Friday 3/8\n",
    "\n",
    "## Multi-class classification via neural networks\n",
    "\n",
    "In this project, you will learn how to implement all the basic components of a neural network, including forward propagation, gradient computation, and back propagation, following the development framework presented by Professor Andrew Ng for [binary classification](https://www.youtube.com/watch?v=eqEc66RFY0I&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=7). The activities in this project will help you gain valuable intuition regarding several of the fundamental programming techniques powering neural network libraries. You will also develop appreciation, through hands-on application, for some of the practical considerations involved in training a neural network. \n",
    "\n",
    "As in Project 1, we will use the MNIST dataset to experiment with binary and multi-class classification problems. \n",
    "\n",
    "### Learning outcomes\n",
    "After completing project 2, you will be able to:\n",
    "* Implement neural networks that use cross-entropy loss for binary classification \n",
    "* Apply ideas originating in binary classification to multi-class classification problems\n",
    "* Describe and apply activation functions, such as sigmoid and softmax\n",
    "* Understand the role of parameters and hyper parameter initialization\n",
    "\n",
    "### Multi-class classification: the MNIST dataset\n",
    "The MNIST dataset consists of 70,000 gray-scale images (samples) of hand-written digits 0 through 9. The multi-class classification problem consists of classifying each sample accurately as belonging to one of ten classes. This dataset is divided into training (60,000) and test (10,000) datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic regression using a neural network implementation framework (50% undergrad, 30% grad)\n",
    "Prof. Andrew Ng's Coursera videos, assigned in Module 2, explain how logistic regression can be implemented as a single neuron that receives images as input and predicts their classification into one of two classes (cats vs. non-cats in his videos). He explains in detail how the process can be separated into a forward pass, calculation of a loss function, and numerical optimization using gradient descent in the back propagation step. \n",
    "\n",
    "Your job for this part of the project is to implement the \"logistic regression with a neural network mindset\" approach described by Professor Ng. For this you, will use a Jupyter notebook provided as part of his Coursera course. A zip file (Logistic Regression as a Neural Network.zip) containing this notebook as well as other files and folders needed can be found as apart of the Google Classroom assignment.\n",
    "\n",
    "#### Implementation requirements (50%)\n",
    "* The Jupyter notebook contains step-by-step implementation instructions. Follow these instructions carefully.\n",
    "* Your code should use the vectorization techniques learned from Prof. Ng's videos. **Pay attention to the order of dimensions of the data matrix X, they are ordered as (features, samples)** \n",
    "* You can use the cat/non-cat dataset to debug your implementation, but it's not required.\n",
    "\n",
    "**Suggestions:** \n",
    "* Always keep the size of your matrices and vectors in mind to avoid confusion.\n",
    "* Include some tests or sanity checks as you've seen in our homework assignment and in Prof. Ng's notebook.\n",
    "* Avoid loops. Learn to use vectorization.\n",
    "\n",
    "#### Application (20%)\n",
    "Can we solve the 10-class MNIST classification problem using our binary classification logistic regression code? The answer is yes. Think about how to reframe the problem so that it can be solved via your binary classification code. Then explain how you are going to do this, discussing the pros and cons of your approach.  \n",
    "\n",
    "\n",
    "#### Results and Analysis (30%)\n",
    "Solve the classification problem and obtain performance results, be sure to specify (and experiment with) the value of $\\alpha$, your hyperparameter. These results should include plots of the cost function value, the training accuracy, and the test accuracy at each iteration (epoch) of your code. Finally, analyze the results you obtained. Here are a few things you might want to consider.\n",
    "\n",
    "* How do the learning cost, the train accuracy, and the test accuracy curves change as function of the learning rate assuming a fix number of iterations (say, 2000 iterations)? \n",
    "* For what range of $\\alpha$ does convergence \"fail\"? \n",
    "* For approximately what values of $\\alpha$ you obtain best performance?\n",
    "\n",
    "**Bonus (5%)**: The weight vector has the same size as the input images, so you can actually reshape it to look like one. What's happening to the weight vector \"image\" as the code converges? What is the algorithm *learning*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extending the framework for multi-class classification (50% undergrad, 70% grad)\n",
    "\n",
    "In this part, we want to solve the 10-class MNIST classification problem via a proper neural network with 10 outputs (i.e. a probability for each class). Here are a couple of ways to extend the idea behind binary classification for the purpose of multi-class classification.  \n",
    " * Option 1. Implement a neural network consisting of one layer containing 10 nodes. Each node will have its own weight vector and bias values. Then, replace the sigmoid activation function with a softmax activation function. The code must put together all 10 weight vectors $\\mathbf{w}_i$ into a matrix $\\mathbf{W}$ and all 10 bias values into a vector $\\mathbf{b}$ and apply vectorization. Prof. Ng goes over how to do this in his C1W3 lectures. \n",
    " * Option 2. Implement a two-layer neural network. The input image goes into all the nodes in the first layer (which contains, say 100 nodes). The output produced by this layer then goes as input to a second layer consisting of 10 nodes and softmax activation function. This is a much more powerful neural network because of the ability of the first layer (the hidden layer) to learn intermediate information about the problem. Prof. Ng lectures also go over in detail about how to obtain the partial derivatives and vectorize the code.\n",
    " \n",
    "\n",
    "**Undergraduate students** You can choose to implement option 1 or 2. (70%)\n",
    "\n",
    "**Grad students** You must implement option 2. (70%)\n",
    "\n",
    "As in part 1, solve the classification problem with your code, obtain and analyze your results. (30%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to turn in \n",
    "\n",
    "You will turn in this assignment via Google Classroom. Let me know if you have any issues so that I can fix those accordingly.\n",
    "\n",
    "What to submit:\n",
    "* Two Jupyter notebooks containing your code for Projects 1 and 2. Be sure to include the generated by each cell.\n",
    "* A report of maximum 4 pages in length in **PDF** containing sections describing: the methodology, the results, and the analysis.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x289bda1b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#packages \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 60000\n",
      "Number of testing samples = 10000\n",
      "Original sample dimensions: 28 x 28\n",
      "Labels (classes): [0 1 2 3 4 5 6 7 8 9]\n",
      "Number of samples per class (training): [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
      "Number of samples per class (testing): [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n",
      "Total number of samples per class (training + testing): [6903 7877 6990 7141 6824 6313 6876 7293 6825 6958]\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# The number of samples in the training and test sets\n",
    "number_train = train_images.shape[0]\n",
    "number_test = test_images.shape[0]\n",
    "# The dimension of each image\n",
    "height = train_images.shape[1]\n",
    "width =train_images.shape[2]\n",
    "unique_values, counts = np.unique(train_labels, return_counts=True)\n",
    "_, counts_test = np.unique(test_labels, return_counts=True)\n",
    "total_samples = counts + counts_test\n",
    "print (f\"Number of training samples = {number_train}\")\n",
    "print (f\"Number of testing samples = {number_test}\")\n",
    "print (f\"Original sample dimensions: {width} x {height}\")\n",
    "print(f\"Labels (classes): {unique_values}\")\n",
    "print (f\"Number of samples per class (training): {counts}\")\n",
    "print (f\"Number of samples per class (testing): {counts_test}\")\n",
    "print(f\"Total number of samples per class (training + testing): {total_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (784, 60000)\n",
      "train_set_y shape: (60000,)\n",
      "test_set_x_flatten shape: (784, 10000)\n",
      "test_set_y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Flattening the data\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "train_set_x_flatten = train_images.reshape(train_images.shape[0], -1).T \n",
    "test_set_x_flatten = test_images.reshape(test_images.shape[0], -1).T \n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_labels.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_labels.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Setting up Helper functions </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for sigmoid(x)\n",
    "def sigmoid(x):\n",
    "    ### START CODE HERE ###\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "#check sigmoid function\n",
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights & bias \n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "     \n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = torch.empty(dim, 1)\n",
    "    torch.nn.init.uniform_(w)\n",
    "    w = w.numpy()\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.4962566]\n",
      " [0.7682218]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "#test initialization function\n",
    "\n",
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    epsilon = 1e-8\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X) + b)    \n",
    "    # compute activation\n",
    "    cost =   (-1/m) * np.sum(Y * np.log(A+epsilon) + (1 - Y) * np.log(1 - A + epsilon))   \n",
    "    # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # print(\"A \", A.shape)\n",
    "    # print(\"Y \", Y.shape)\n",
    "    dz = A-Y \n",
    "    # dw = (1/m) * X * dz.T\n",
    "    dw = (1/m) * np.dot(X,dz.T)\n",
    "    db = (1/m) * np.sum(dz)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801002496414526\n"
     ]
    }
   ],
   "source": [
    "#test propagation function\n",
    "\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "# print(w)\n",
    "# print(b)\n",
    "# print(X)\n",
    "# print(Y)\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimization\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = propagate(w=w, b=b, X=X, Y=Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - (learning_rate * dw)\n",
    "        b = b - (learning_rate * db)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.19033591]\n",
      " [0.12259159]]\n",
      "b = 1.9253598300845747\n",
      "dw = [[0.67752042]\n",
      " [1.41625495]]\n",
      "db = 0.21919450454067652\n"
     ]
    }
   ],
   "source": [
    "#test optimization function\n",
    "\n",
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    # print(\"X shape\", X.shape)\n",
    "    # print(\"W shape \", w.shape )\n",
    "    # print(\"WT shape \", w.T.shape )\n",
    "    # w = w.reshape(X.shape[0], 1)\n",
    "   \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b) \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        if A[0,i] >= 0.75:\n",
    "            Y_prediction[0,i] = 1\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        pass\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model \n",
    "\n",
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "#     print(\"new w \", w)\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w=w, b=b, X=X_train, Y=Y_train, learning_rate=learning_rate, num_iterations=num_iterations)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_cost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 28\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     25\u001b[0m w, b \u001b[38;5;241m=\u001b[39m initialize_with_zeros(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Gradient descent (≈ 1 line of code)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m parameters, grads, costs \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_iterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Retrieve parameters w and b from dictionary \"parameters\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m w \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(w, b, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     27\u001b[0m costs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m     30\u001b[0m     \n\u001b[1;32m     31\u001b[0m     \n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Cost and gradient calculation (≈ 1-4 lines of code)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m### START CODE HERE ### \u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     grads, cost \u001b[38;5;241m=\u001b[39m \u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Retrieve derivatives from grads\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     dw \u001b[38;5;241m=\u001b[39m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdw\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m     35\u001b[0m dz \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m-\u001b[39mY \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# dw = (1/m) * X * dz.T\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m dw \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m db \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mm) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dz)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_labels, test_set_x, test_labels, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 98.56459330143541 %\n",
      "test accuracy: 76.0 %\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# sys.path(\"./\")\n",
    "from lr_utils import load_dataset\n",
    "\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T \n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T \n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255.\n",
    "\n",
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 5000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    # Lists to store samples and labels\n",
    "samples_current_class = []\n",
    "labels_current_class = []\n",
    "samples_other_classes = []\n",
    "labels_other_classes = []\n",
    "\n",
    "# Define the number of samples to grab\n",
    "num_samples = 5000\n",
    "\n",
    "# Loop through each digit\n",
    "for digit in range(10):\n",
    "    # Get indices of samples for current digit\n",
    "    digit_indices = np.where(train_labels == digit)[0]\n",
    "    \n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(digit_indices)\n",
    "    \n",
    "    # Grab 5000 samples of the current digit\n",
    "    current_digit_samples = digit_indices[:num_samples]\n",
    "    \n",
    "    # Grab 5000 samples of other digits\n",
    "    other_digits_indices = np.where(train_labels != digit)[0]\n",
    "    np.random.shuffle(other_digits_indices)\n",
    "    other_digits_samples = other_digits_indices[:num_samples]\n",
    "    \n",
    "    # Append samples and labels to lists\n",
    "    samples_current_class.extend(train_images[current_digit_samples])\n",
    "    labels_current_class.extend(train_labels[current_digit_samples])\n",
    "    samples_other_classes.extend(train_images[other_digits_samples])\n",
    "    labels_other_classes.extend(train_labels[other_digits_samples])\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "samples_current_class = np.array(samples_current_class)\n",
    "labels_current_class = np.array(labels_current_class)\n",
    "samples_other_classes = np.array(samples_other_classes)\n",
    "labels_other_classes = np.array(labels_other_classes)\n",
    "    #grab 5k samples of the current target class (digit)\n",
    "    \n",
    "    #grab 5k samples of the other randomly classes \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store samples and labels\n",
    "samples_current_class = []\n",
    "labels_current_class = []\n",
    "samples_other_classes = []\n",
    "labels_other_classes = []\n",
    "\n",
    "# Define the number of samples to grab\n",
    "num_samples = 5000\n",
    "\n",
    "# Loop through each digit\n",
    "for digit in range(10):\n",
    "    # Get indices of samples for current digit\n",
    "    digit_indices = np.where(train_labels == digit)[0]\n",
    "    \n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(digit_indices)\n",
    "    \n",
    "    # Grab 5000 samples of the current digit\n",
    "    current_digit_samples = digit_indices[:num_samples]\n",
    "    \n",
    "    # Grab 5000 samples of other digits\n",
    "    other_digits_indices = np.where(train_labels != digit)[0]\n",
    "    np.random.shuffle(other_digits_indices)\n",
    "    other_digits_samples = other_digits_indices[:num_samples]\n",
    "    \n",
    "    # Append samples and labels to lists\n",
    "    samples_current_class.extend(train_images[current_digit_samples])\n",
    "    labels_current_class.extend(train_labels[current_digit_samples])\n",
    "    samples_other_classes.extend(train_images[other_digits_samples])\n",
    "    labels_other_classes.extend(train_labels[other_digits_samples])\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "samples_current_class = np.array(samples_current_class)\n",
    "labels_current_class = np.array(labels_current_class)\n",
    "samples_other_classes = np.array(samples_other_classes)\n",
    "labels_other_classes = np.array(labels_other_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store samples and labels for each digit\n",
    "samples_by_digit = {}\n",
    "labels_by_digit = {}\n",
    "\n",
    "# Define the number of samples to grab\n",
    "num_samples = 5000\n",
    "\n",
    "# Loop through each digit\n",
    "for digit in range(10):\n",
    "    # Get indices of samples for current digit\n",
    "    digit_indices = np.where(train_labels == digit)[0]\n",
    "    \n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(digit_indices)\n",
    "    \n",
    "    # Grab 5000 samples of the current digit\n",
    "    digit_samples = train_images[digit_indices[:num_samples]]\n",
    "    digit_labels = train_labels[digit_indices[:num_samples]]\n",
    "    \n",
    "    # Store samples and labels for the current digit\n",
    "    samples_by_digit[digit] = digit_samples\n",
    "    labels_by_digit[digit] = digit_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "\n",
    "# Function to create the dataset for a given digit\n",
    "(train_images_1, train_labels_1), _ = mnist.load_data()\n",
    "def create_dataset_for_digit(digit):\n",
    "    \n",
    "    train_images = deepcopy(train_images_1)\n",
    "    train_labels = deepcopy(train_labels_1)\n",
    "# Flatten the images if necessary\n",
    "    train_images = train_images.reshape((train_images.shape[0], -1))\n",
    "\n",
    "    # Normalize the images\n",
    "    train_images = train_images / 255.0\n",
    "    # Filter images for the current digit\n",
    "    current_digit_images = deepcopy(train_images[train_labels == digit])\n",
    "    current_digit_labels = train_labels[train_labels == digit]\n",
    "    \n",
    "    # Sample 5000 images if necessary\n",
    "    if len(current_digit_images) > 5000:\n",
    "        indices = np.random.choice(range(len(current_digit_images)), 5000, replace=False)\n",
    "        current_digit_images = current_digit_images[indices]\n",
    "        current_digit_labels = current_digit_labels[indices]\n",
    "    \n",
    "    # Images for other digits\n",
    "    other_digits_images = []\n",
    "    other_digits_labels = []\n",
    "    \n",
    "    # Number of samples per other digit to make up 5000 images\n",
    "    samples_per_digit = 5000 // 9\n",
    "    # print(\"sp\", samples_per_digit)\n",
    "    for other_digit in range(10):\n",
    "        if other_digit != digit:\n",
    "            # Filter images for the other digit\n",
    "            images = train_images[train_labels == other_digit]\n",
    "            labels = train_labels[train_labels == other_digit]\n",
    "            \n",
    "            # Sample images\n",
    "            if len(images) > samples_per_digit:\n",
    "                indices = np.random.choice(range(len(images)), samples_per_digit, replace=False)\n",
    "                images = images[indices]\n",
    "                labels = labels[indices]\n",
    "            \n",
    "            other_digits_images.append(images)\n",
    "            other_digits_labels.append(labels)\n",
    "    \n",
    "    # Combine the other digits' images and labels\n",
    "    other_digits_images = np.concatenate(other_digits_images, axis=0)\n",
    "    other_digits_labels = np.concatenate(other_digits_labels, axis=0)\n",
    "    \n",
    "    # Combine current digit with other digits to form the dataset\n",
    "    combined_images = np.concatenate((current_digit_images, other_digits_images), axis=0)\n",
    "    combined_labels = np.concatenate((current_digit_labels, other_digits_labels), axis=0)\n",
    "    \n",
    "    return combined_images, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset for digit 7 with shape (5999, 784) and labels shape (5999,)\n"
     ]
    }
   ],
   "source": [
    "#Test Code for datset split\n",
    "# Example usage for digit 0\n",
    "digit = 7\n",
    "images, labels = create_dataset_for_digit(digit)\n",
    "\n",
    "print(f\"Created dataset for digit {digit} with shape {images.shape} and labels shape {labels.shape}\")\n",
    "\n",
    "# Example of a picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle returned data \n",
    "import random \n",
    "def create_dataset(digit):\n",
    "    # Your function that returns samples and labels for the given digit\n",
    "    # For demonstration, let's assume it returns two lists: samples and labels\n",
    "      # Assuming this contains your labels\n",
    "    samples, labels = create_dataset_for_digit(digit)\n",
    "    # Shuffle samples and labels together\n",
    "    shuffled_indices = np.random.permutation(len(samples))\n",
    "    # print(\"Shuffled_indices \", shuffled_indices)\n",
    "    shuffled_samples = samples[shuffled_indices]\n",
    "    shuffled_labels = labels[shuffled_indices]\n",
    "    \n",
    "    return shuffled_samples, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_test(digit):\n",
    "    # Your function that returns samples and labels for the given digit\n",
    "    # For demonstration, let's assume it returns two lists: samples and labels\n",
    "      # Assuming this contains your labels\n",
    "    samples, labels = create_dataset_for_digit_test(digit)\n",
    "    # Shuffle samples and labels together\n",
    "    shuffled_indices = np.random.permutation(len(samples))\n",
    "    shuffled_samples = samples[shuffled_indices]\n",
    "    shuffled_labels = labels[shuffled_indices]\n",
    "    \n",
    "    return shuffled_samples, shuffled_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "_, (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Flatten the images if necessary\n",
    "test_images = test_images.reshape((test_images.shape[0], -1))\n",
    "\n",
    "# Normalize the images\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Function to create the dataset for a given digit\n",
    "def create_dataset_for_digit_test(digit):\n",
    "    # Filter images for the current digit\n",
    "    current_digit_images = test_images[test_labels == digit]\n",
    "    current_digit_labels = test_labels[test_labels == digit]\n",
    "    \n",
    "    # Sample 5000 images if necessary\n",
    "    if len(current_digit_images) > 100:\n",
    "        indices = np.random.choice(range(len(current_digit_images)), 300, replace=False)\n",
    "        current_digit_images = current_digit_images[indices]\n",
    "        current_digit_labels = current_digit_labels[indices]\n",
    "    \n",
    "    # Images for other digits\n",
    "    other_digits_images = []\n",
    "    other_digits_labels = []\n",
    "    \n",
    "    # Number of samples per other digit to make up 5000 images\n",
    "    samples_per_digit = 300 // 9\n",
    "    \n",
    "    for other_digit in range(10):\n",
    "        if other_digit != digit:\n",
    "            # Filter images for the other digit\n",
    "            images = test_images[test_labels == other_digit]\n",
    "            labels = test_labels[test_labels == other_digit]\n",
    "            \n",
    "            # Sample images\n",
    "            if len(images) > samples_per_digit:\n",
    "                indices = np.random.choice(range(len(images)), samples_per_digit, replace=False)\n",
    "                images = images[indices]\n",
    "                labels = labels[indices]\n",
    "            \n",
    "            other_digits_images.append(images)\n",
    "            other_digits_labels.append(labels)\n",
    "    \n",
    "    # Combine the other digits' images and labels\n",
    "    other_digits_images = np.concatenate(other_digits_images, axis=0)\n",
    "    other_digits_labels = np.concatenate(other_digits_labels, axis=0)\n",
    "    \n",
    "    # Combine current digit with other digits to form the dataset\n",
    "    combined_images = np.concatenate((current_digit_images, other_digits_images), axis=0)\n",
    "    combined_labels = np.concatenate((current_digit_labels, other_digits_labels), axis=0)\n",
    "    \n",
    "    return combined_images, combined_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  1\n",
      "Shuffled_indices  [4992 9475 6069 ... 1548 5469 3435]\n",
      "train accuracy: 96.72836418209104 %\n",
      "test accuracy: 98.22 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  2\n",
      "Shuffled_indices  [2791  940 8011 ... 1708 5231 8242]\n",
      "train accuracy: 90.11505752876438 %\n",
      "test accuracy: 92.28 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  3\n",
      "Shuffled_indices  [1593  659 3752 ... 8956 8190 2067]\n",
      "train accuracy: 89.6048024012006 %\n",
      "test accuracy: 90.94 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  4\n",
      "Shuffled_indices  [3515 1707 4741 ...  809 8798 9152]\n",
      "train accuracy: 87.15357678839419 %\n",
      "test accuracy: 87.36 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  5\n",
      "Shuffled_indices  [4611 3847 3480 ... 2333 2586 2025]\n",
      "train accuracy: 83.02151075537769 %\n",
      "test accuracy: 86.51 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  6\n",
      "Shuffled_indices  [1533 6054 2273 ... 6195 4337 6344]\n",
      "train accuracy: 92.76638319159579 %\n",
      "test accuracy: 91.13 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  7\n",
      "Shuffled_indices  [6644 8080 4397 ... 1692 1161 7588]\n",
      "train accuracy: 91.9959979989995 %\n",
      "test accuracy: 92.25 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  8\n",
      "Shuffled_indices  [ 560 2608 2099 ... 9193   52 6537]\n",
      "train accuracy: 86.5632816408204 %\n",
      "test accuracy: 89.51 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  9\n",
      "Shuffled_indices  [2689  206 5899 ... 9165 5878 9716]\n",
      "train accuracy: 86.78339169584793 %\n",
      "test accuracy: 87.93 %\n",
      "cur  [7 2 1 ... 4 5 6]\n",
      "Eval digit  10\n",
      "Shuffled_indices  [2577 2766 3254 ... 5451 1264  608]\n",
      "train accuracy: 98.16216216216216 %\n",
      "test accuracy: 97.73 %\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "_, (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Flatten the images if necessary\n",
    "test_images = test_images.reshape((test_images.shape[0], -1))\n",
    "\n",
    "# Normalize the images\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    i=i+1 #skipping digit 0 for now\n",
    "    \n",
    "    if i==0:\n",
    "        \n",
    "    \n",
    "    curr_test_labels = deepcopy(test_labels)\n",
    "   \n",
    "    curr_test_labels [ curr_test_labels == i ]= 1\n",
    "    curr_test_labels [ curr_test_labels != 1 ]= 0\n",
    "    \n",
    "    shuffled_samples, shuffled_labels = create_dataset(i)\n",
    "    shuffled_samples_test, shuffled_labels_test = create_dataset_test(i)\n",
    "   \n",
    "  \n",
    "\n",
    "    shuffled_labels[shuffled_labels== i] = 1\n",
    "    shuffled_labels[shuffled_labels!= 1] = 0\n",
    "    shuffled_labels_test[shuffled_labels_test!=0] = 1\n",
    "    f = open(\"./log_1.txt\", \"a\")\n",
    "    f.write(str(shuffled_labels) + f\"{i} \\n\")\n",
    "    f.close()\n",
    "    # print(shuffled_samples.shape)\n",
    "    # print(shuffled_samples_test.shape)\n",
    "    #we need to pass the transpose because of the way I'm splliting and shuflling the data we get (x, 784) and we want it to be (784, X) \n",
    "    model(shuffled_samples.T, shuffled_labels, test_images.T, curr_test_labels, num_iterations = 1000, learning_rate = 0.05, print_cost = True)\n",
    "    # print(\"d\" ,d['w'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
