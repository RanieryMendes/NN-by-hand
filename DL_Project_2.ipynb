{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project 2 \n",
    "\n",
    "* Due latest by Friday 3/8\n",
    "\n",
    "## Multi-class classification via neural networks\n",
    "\n",
    "In this project, you will learn how to implement all the basic components of a neural network, including forward propagation, gradient computation, and back propagation, following the development framework presented by Professor Andrew Ng for [binary classification](https://www.youtube.com/watch?v=eqEc66RFY0I&list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0&index=7). The activities in this project will help you gain valuable intuition regarding several of the fundamental programming techniques powering neural network libraries. You will also develop appreciation, through hands-on application, for some of the practical considerations involved in training a neural network. \n",
    "\n",
    "As in Project 1, we will use the MNIST dataset to experiment with binary and multi-class classification problems. \n",
    "\n",
    "### Learning outcomes\n",
    "After completing project 2, you will be able to:\n",
    "* Implement neural networks that use cross-entropy loss for binary classification \n",
    "* Apply ideas originating in binary classification to multi-class classification problems\n",
    "* Describe and apply activation functions, such as sigmoid and softmax\n",
    "* Understand the role of parameters and hyper parameter initialization\n",
    "\n",
    "### Multi-class classification: the MNIST dataset\n",
    "The MNIST dataset consists of 70,000 gray-scale images (samples) of hand-written digits 0 through 9. The multi-class classification problem consists of classifying each sample accurately as belonging to one of ten classes. This dataset is divided into training (60,000) and test (10,000) datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic regression using a neural network implementation framework (50% undergrad, 30% grad)\n",
    "Prof. Andrew Ng's Coursera videos, assigned in Module 2, explain how logistic regression can be implemented as a single neuron that receives images as input and predicts their classification into one of two classes (cats vs. non-cats in his videos). He explains in detail how the process can be separated into a forward pass, calculation of a loss function, and numerical optimization using gradient descent in the back propagation step. \n",
    "\n",
    "Your job for this part of the project is to implement the \"logistic regression with a neural network mindset\" approach described by Professor Ng. For this you, will use a Jupyter notebook provided as part of his Coursera course. A zip file (Logistic Regression as a Neural Network.zip) containing this notebook as well as other files and folders needed can be found as apart of the Google Classroom assignment.\n",
    "\n",
    "#### Implementation requirements (50%)\n",
    "* The Jupyter notebook contains step-by-step implementation instructions. Follow these instructions carefully.\n",
    "* Your code should use the vectorization techniques learned from Prof. Ng's videos. **Pay attention to the order of dimensions of the data matrix X, they are ordered as (features, samples)** \n",
    "* You can use the cat/non-cat dataset to debug your implementation, but it's not required.\n",
    "\n",
    "**Suggestions:** \n",
    "* Always keep the size of your matrices and vectors in mind to avoid confusion.\n",
    "* Include some tests or sanity checks as you've seen in our homework assignment and in Prof. Ng's notebook.\n",
    "* Avoid loops. Learn to use vectorization.\n",
    "\n",
    "#### Application (20%)\n",
    "Can we solve the 10-class MNIST classification problem using our binary classification logistic regression code? The answer is yes. Think about how to reframe the problem so that it can be solved via your binary classification code. Then explain how you are going to do this, discussing the pros and cons of your approach.  \n",
    "\n",
    "\n",
    "#### Results and Analysis (30%)\n",
    "Solve the classification problem and obtain performance results, be sure to specify (and experiment with) the value of $\\alpha$, your hyperparameter. These results should include plots of the cost function value, the training accuracy, and the test accuracy at each iteration (epoch) of your code. Finally, analyze the results you obtained. Here are a few things you might want to consider.\n",
    "\n",
    "* How do the learning cost, the train accuracy, and the test accuracy curves change as function of the learning rate assuming a fix number of iterations (say, 2000 iterations)? \n",
    "* For what range of $\\alpha$ does convergence \"fail\"? \n",
    "* For approximately what values of $\\alpha$ you obtain best performance?\n",
    "\n",
    "**Bonus (5%)**: The weight vector has the same size as the input images, so you can actually reshape it to look like one. What's happening to the weight vector \"image\" as the code converges? What is the algorithm *learning*?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extending the framework for multi-class classification (50% undergrad, 70% grad)\n",
    "\n",
    "In this part, we want to solve the 10-class MNIST classification problem via a proper neural network with 10 outputs (i.e. a probability for each class). Here are a couple of ways to extend the idea behind binary classification for the purpose of multi-class classification.  \n",
    " * Option 1. Implement a neural network consisting of one layer containing 10 nodes. Each node will have its own weight vector and bias values. Then, replace the sigmoid activation function with a softmax activation function. The code must put together all 10 weight vectors $\\mathbf{w}_i$ into a matrix $\\mathbf{W}$ and all 10 bias values into a vector $\\mathbf{b}$ and apply vectorization. Prof. Ng goes over how to do this in his C1W3 lectures. \n",
    " * Option 2. Implement a two-layer neural network. The input image goes into all the nodes in the first layer (which contains, say 100 nodes). The output produced by this layer then goes as input to a second layer consisting of 10 nodes and softmax activation function. This is a much more powerful neural network because of the ability of the first layer (the hidden layer) to learn intermediate information about the problem. Prof. Ng lectures also go over in detail about how to obtain the partial derivatives and vectorize the code.\n",
    " \n",
    "\n",
    "**Undergraduate students** You can choose to implement option 1 or 2. (70%)\n",
    "\n",
    "**Grad students** You must implement option 2. (70%)\n",
    "\n",
    "As in part 1, solve the classification problem with your code, obtain and analyze your results. (30%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to turn in \n",
    "\n",
    "You will turn in this assignment via Google Classroom. Let me know if you have any issues so that I can fix those accordingly.\n",
    "\n",
    "What to submit:\n",
    "* Two Jupyter notebooks containing your code for Projects 1 and 2. Be sure to include the generated by each cell.\n",
    "* A report of maximum 4 pages in length in **PDF** containing sections describing: the methodology, the results, and the analysis.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28b0e9e70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#packages \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(0)\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples = 60000\n",
      "Number of testing samples = 10000\n",
      "Original sample dimensions: 28 x 28\n",
      "Labels (classes): [0 1 2 3 4 5 6 7 8 9]\n",
      "Number of samples per class (training): [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
      "Number of samples per class (testing): [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n",
      "Total number of samples per class (training + testing): [6903 7877 6990 7141 6824 6313 6876 7293 6825 6958]\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "# The number of samples in the training and test sets\n",
    "number_train = train_images.shape[0]\n",
    "number_test = test_images.shape[0]\n",
    "# The dimension of each image\n",
    "height = train_images.shape[1]\n",
    "width =train_images.shape[2]\n",
    "unique_values, counts = np.unique(train_labels, return_counts=True)\n",
    "_, counts_test = np.unique(test_labels, return_counts=True)\n",
    "total_samples = counts + counts_test\n",
    "print (f\"Number of training samples = {number_train}\")\n",
    "print (f\"Number of testing samples = {number_test}\")\n",
    "print (f\"Original sample dimensions: {width} x {height}\")\n",
    "print(f\"Labels (classes): {unique_values}\")\n",
    "print (f\"Number of samples per class (training): {counts}\")\n",
    "print (f\"Number of samples per class (testing): {counts_test}\")\n",
    "print(f\"Total number of samples per class (training + testing): {total_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (784, 60000)\n",
      "train_set_y shape: (60000,)\n",
      "test_set_x_flatten shape: (784, 10000)\n",
      "test_set_y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Flattening the data\n",
    "\n",
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "train_set_x_flatten = train_images.reshape(train_images.shape[0], -1).T \n",
    "test_set_x_flatten = test_images.reshape(test_images.shape[0], -1).T \n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_labels.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_labels.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Setting up Helper functions </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition for sigmoid(x)\n",
    "def sigmoid(x):\n",
    "    ### START CODE HERE ###\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0, 2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "#check sigmoid function\n",
    "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize weights & bias \n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "     \n",
    "\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    w = torch.empty(dim, 1)\n",
    "    torch.nn.init.uniform_(w)\n",
    "    w = w.numpy()\n",
    "    b = 0\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.4962566]\n",
      " [0.7682218]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "#test initialization function\n",
    "\n",
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)    \n",
    "    print(A)# compute activation\n",
    "    cost =   (-1/m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))   \n",
    "    # compute cost\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dz = A-Y \n",
    "    # dw = (1/m) * X * dz.T\n",
    "    dw = (1/m) * np.dot(X,dz.T)\n",
    "    db = (1/m) * np.sum(dz)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99987661 0.99999386 0.00449627]]\n",
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "#test propagation function\n",
    "\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "# print(w)\n",
    "# print(b)\n",
    "# print(X)\n",
    "# print(Y)\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
